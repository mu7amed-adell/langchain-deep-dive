{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df82a29",
   "metadata": {},
   "source": [
    "### Why split documents?  [reference](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/text_splitters.mdx#why-split-documents)\n",
    "There are several reasons to split documents:\n",
    "\n",
    "- Handling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\n",
    "- Overcoming model limitations: Many embedding models and language models have maximum input size constraints. - Splitting allows us to process documents that would otherwise exceed these limits.\n",
    "- Improving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\n",
    "- Enhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\n",
    "- Optimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\n",
    "\n",
    "See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c498e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ceec",
   "metadata": {},
   "source": [
    "### What is the goal of chunking?\n",
    "As Greg Kamradt's says in [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb): \n",
    "\"Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\"\n",
    "\n",
    "### The Challenge of Chunking\n",
    "Chunking can split a paragraph or a sentence in half, which may cause the text to lose its semantic meaning. This fragmentation can make it harder for retrieval systems to understand and fetch relevant information accurately.\n",
    "\n",
    "### Why is Proper Chunking Important?\n",
    "1. **Semantic Integrity**: Ensures each chunk retains enough context to be meaningful on its own\n",
    "2. **Retrieval Quality**: Affects how well your RAG system can find relevant information\n",
    "3. **Model Performance**: Impacts how effectively LLMs can process and generate responses\n",
    "\n",
    "### Practical Solutions (as shown in later examples):\n",
    "- **Recursive Splitting**: Breaks down text hierarchically (documents → paragraphs → sentences)\n",
    "- **Overlap Strategies**: Maintains context between chunks (as demonstrated with `chunk_overlap=4`)\n",
    "- **Custom Separators**: Uses natural boundaries like `\\n\\n` for paragraphs or spaces for words\n",
    "\n",
    "As we'll see in the following cells, LangChain's text splitters provide these capabilities out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0c948",
   "metadata": {},
   "source": [
    "---\n",
    "### Recursive and Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ae94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "chunk=26\n",
    "overlap=4\n",
    "\n",
    "RecursiveSplitter = RecursiveCharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "CharacterSplitter = CharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf5175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['abcdefghijklmnopqrstuvwxyz']\n",
      "CharacterSplitter: ['abcdefghijklmnopqrstuvwxyz']\n"
     ]
    }
   ],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text1))\n",
    "print('CharacterSplitter:', CharacterSplitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbf5cf",
   "metadata": {},
   "source": [
    "What is happening? \n",
    "The Recursive splitter is splitting the alphabetics according to the chunk size (26) and the overlap (4), thats why there is no splitting for the 26 characters of the alphabetics.\n",
    "\n",
    "The Character splitter on the other hand, is looking for the default separator to split. The default separator is the (\\n\\n) doubble new line. which in our example text1 does not exist. therefore, it all appear to the splitter as a single chunk.\n",
    "\n",
    "lets try another text. without changing any values just the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26d255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'Mohamed Adel Hassan Ismaiel'\n",
    "len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9c015",
   "metadata": {},
   "source": [
    "my name is 27 characters long including the spaces, lets try and look how would the splitters handle this simple case. to further understand it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b98cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan Ismaiel']\n"
     ]
    }
   ],
   "source": [
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text2))\n",
    "print('CharacterSplitter:', CharacterSplitter.split_text(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f10773",
   "metadata": {},
   "source": [
    "As I guessed, hope you did too. The Recursive splitter splitted after the 20th character. bacause if the 'ismaiel' was also included it would have exceeded the chunk size of 26. simple!\n",
    "\n",
    "Why doesn't the character splitter split the string?\n",
    "```\n",
    "Character splitter includes /n/n separator by default. Therefore, in this example the text has no new lines. Therefore, it appears to the Character splitter as one single string. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7742baec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan', 'Ismaiel']\n"
     ]
    }
   ],
   "source": [
    "# lets adjust the separator value to be ' ' a single space.\n",
    "c_split = CharacterTextSplitter(chunk_size=chunk, chunk_overlap= overlap, separator=' ')\n",
    "\n",
    "print('RecursiveSplitter:', RecursiveSplitter.split_text(text2))\n",
    "print('CharacterSplitter:', c_split.split_text(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67652b62",
   "metadata": {},
   "source": [
    "After setting the separator value ' ', they both return the same split. However this is just pure luck because of the text example. How does the RecursiveCharacterTextSplitter actually works?  Recursive Splitter does not use just a single ' ' (space) as its default separator. Instead, it uses a list of separators in order of priority, and it will try to split on the “largest meaningful” one first. \n",
    "```\n",
    "separators = [\n",
    "    \"\\n\\n\",   # paragraph breaks\n",
    "    \"\\n\",    # line breaks\n",
    "    \" \",     # spaces\n",
    "    \"\"       # as a last resort, split by character\n",
    "]\n",
    "```\n",
    "\n",
    "If you think of it for a second, for longer texts, Recursive gives “semantic” splits (paragraphs/lines/words fallback), while Character is rigid (always space / according to the value set for the separator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a06f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveSplitter: ['Mohamed Adel Hassan', 'Ismaiel', 'Lives in Cairo', 'Works as an AI engineer', 'Enjoys AI, Python, and', 'and teaching']\n",
      "CharacterSplitter: ['Mohamed Adel Hassan Ismaiel\\nLives in Cairo\\nWorks as an AI engineer\\nEnjoys AI, Python, and teaching']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text3 = \"\"\"Mohamed Adel Hassan Ismaiel\n",
    "Lives in Cairo\n",
    "Works as an AI engineer\n",
    "Enjoys AI, Python, and teaching\"\"\"\n",
    "\n",
    "chunk=26\n",
    "overlap=4\n",
    "\n",
    "r_split = RecursiveCharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "c_split = CharacterTextSplitter(chunk_size=chunk, chunk_overlap=overlap)\n",
    "\n",
    "print(\"RecursiveSplitter:\", r_split.split_text(text3))\n",
    "print(\"CharacterSplitter:\", c_split.split_text(text3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48221ea5",
   "metadata": {},
   "source": [
    "you may have a question in mind, let me say it out loud. WHY ISN'T THERE ANY OVERLAP???\n",
    "\n",
    "How RecursiveCharacterTextSplitter decides to split\n",
    "\n",
    "It tries separators in order: [\"\\n\\n\", \"\\n\", \" \", \"\"] (paragraph → newline → space → character). It prefers the largest semantic splits first. \n",
    "LangChain\n",
    "\n",
    "If a separator produces pieces that are each ≤ chunk_size, those pieces are used as chunks and the splitter does not break them further — and therefore no overlap is produced between them. Overlap is only visible when a single semantic piece must itself be split into multiple sub-chunks (then those sub-chunks can be made with the requested chunk_overlap). This is the behavior many people stumble over. [Stackoverflow source](https://stackoverflow.com/questions/76681318/why-is-recursivecharactertextsplitter-not-giving-any-chunk-overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b98820",
   "metadata": {},
   "source": [
    "### I guess now we understood how everything works at the backend. lets now try out some real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e99e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_text = \"\"\"When reading documents, readers rely on structure to understand the flow of information. \\\n",
    "Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. \\\n",
    "For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. \\\n",
    "This organization helps readers follow complex ideas step by step. \\n\\n  \\\n",
    "Formatting cues like bold or italic text also add meaning. \\\n",
    "They emphasize important words or phrases that the writer wants to highlight. \\\n",
    "Lists, whether numbered or bulleted, show relationships between items in a clear manner. \\\n",
    "Altogether, these features create a document that is easier to scan, read, and comprehend.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c0ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] #default values, shown only to make it visible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec57ee5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When reading documents, readers rely on structure to understand the flow of information. Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. This organization helps readers follow complex ideas step by step. \\n\\n Formatting cues like bold or italic text also add meaning. They emphasize important words',\n",
       " 'or phrases that the writer wants to highlight. Lists, whether numbered or bulleted, show relationships between items in a clear manner. Altogether, these features create a document that is easier to scan, read, and comprehend.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(more_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7314ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When reading documents, readers rely on structure to understand the flow of information. Headings, subheadings, and paragraphs provide signposts that guide the reader through the material. For instance, a heading may introduce a new topic, while a subheading narrows the focus to a detail. This organization helps readers follow complex ideas step by step.',\n",
       " 'Formatting cues like bold or italic text also add meaning. They emphasize important words or phrases that the writer wants to highlight. Lists, whether numbered or bulleted, show relationships between items in a clear manner. Altogether, these features create a document that is easier to scan, read, and comprehend.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(more_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094408a7",
   "metadata": {},
   "source": [
    "```\n",
    "To finalize, character split is dumb, it splits according to the chunk_size and the seperator if assigned.\n",
    "Recursive split algorithm prioritizes the semantic meaning, think of it like the order of operations in math (PEMDAS). it first applies the \\n\\n which divides according to paragraphs, if the chunk_size is still larger, then it will split on sentences, if still bigger than the specified chunk size, it will split words, so on... \n",
    "\n",
    "This algorithm makes it preserve as much context of each related paragraphs together, avoiding awkward splits within paragraph or sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d364b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf')\n",
    "pdf = loader.load()\n",
    "len(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9615ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_split = CharacterTextSplitter(chunk_size=450, chunk_overlap=150, separator='\\n')\n",
    "docs = character_split.split_documents(pdf)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0210e",
   "metadata": {},
   "source": [
    "### Let's try out passing the relevant chunks to an LLM. let's see the output when passing the relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c10f3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "persistent_dir = os.path.join(current_dir, \"db\", \"char_db\")\n",
    "embedding = OllamaEmbeddings(model='nomic-embed-text:v1.5')\n",
    "db = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=persistent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8fca4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the modules explained in this class ?\"\n",
    "results = db.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306740bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Document 2:\n",
      "okay?  \n",
      "So as an overview of what we're going to do in this class, this class is sort of organized \n",
      "into four major sections. We're gonna talk about four major topics in this class, the first \n",
      "of which is supervised learning. So let me give you an example of that.  \n",
      "So suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \n",
      "actually collected a data set for me last week to use in the example later. But suppose that\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Document 3:\n",
      "done in MATLAB or Octave, and let's see. And I guess the program prerequisites is more \n",
      "the ability to understand big?O notation and knowledge of what a data structure, like a \n",
      "linked list or a queue or binary treatments, more so than your knowledge of C or Java \n",
      "specifically. Yeah?  \n",
      "Student : Looking at the end semester project, I mean, what exactly will you be testing \n",
      "over there? [Inaudible]?  \n",
      "Instructor (Andrew Ng) : Of the project?\n",
      "\n",
      "Source: D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf\n",
      "\n",
      "Here are some documents that might help answer the question: What is the modules explained in this class ?\n",
      "\n",
      "Relevant Documents:\n",
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so\n",
      "\n",
      "okay?  \n",
      "So as an overview of what we're going to do in this class, this class is sort of organized \n",
      "into four major sections. We're gonna talk about four major topics in this class, the first \n",
      "of which is supervised learning. So let me give you an example of that.  \n",
      "So suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \n",
      "actually collected a data set for me last week to use in the example later. But suppose that\n",
      "\n",
      "done in MATLAB or Octave, and let's see. And I guess the program prerequisites is more \n",
      "the ability to understand big?O notation and knowledge of what a data structure, like a \n",
      "linked list or a queue or binary treatments, more so than your knowledge of C or Java \n",
      "specifically. Yeah?  \n",
      "Student : Looking at the end semester project, I mean, what exactly will you be testing \n",
      "over there? [Inaudible]?  \n",
      "Instructor (Andrew Ng) : Of the project?\n",
      "\n",
      "Please provide an answer based only on the provided documents. Never mention the documents in your response. If the answer is not found in the documents, respond with 'I'm not sure'.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
    "        \n",
    "combined_user_query = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([result.page_content for result in results])\n",
    "    + \"\\n\\nPlease provide an answer based only on the provided documents. Never mention the documents in your response. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
    ")\n",
    "print(combined_user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd9e2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on what's been explained, it seems that one of the topics covered in the class is supervised learning, specifically an example involving housing prices data sets. However, there isn't a clear mention of the specific modules or topics that will be covered throughout the class. The instructor mentions that the class is organized into four major sections, but these are not explicitly stated.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "ollama = ChatOllama(model='llama3.2:3b')\n",
    "## create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant named Gulia.\"),\n",
    "    HumanMessage(content=combined_user_query),\n",
    "]\n",
    "result = ollama.invoke(messages)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baad659",
   "metadata": {},
   "source": [
    "### Markdonw Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e4491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "\n",
    "Header_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90c674d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Driving'}, page_content='Try driving on the 1 down to San Diego'),\n",
       " Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Driving', 'Header 3': 'Food'}, page_content=\"Make sure to eat a burrito while you're there\"),\n",
       " Document(metadata={'Header 1': 'Fun in California', 'Header 2': 'Hiking'}, page_content='Go to Yosemite')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "md_split = MarkdownHeaderTextSplitter(headers_to_split_on=Header_to_split_on)\n",
    "md_split_doc = md_split.split_text(md_text)\n",
    "md_split_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe73a1",
   "metadata": {},
   "source": [
    "- MarkdownHeaderTextSplitter parses a Markdown document.\n",
    "\n",
    "- It splits text based on headers you define in headers_to_split_on.\n",
    "\n",
    "- Each resulting chunk is wrapped in a Document object with:\n",
    "\n",
    "    *   page_content → the text under that section.\n",
    "\n",
    "    *   metadata → a dictionary storing the header hierarchy (e.g., H1, H2, H3).\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Each Document retains the header hierarchy in metadata.\n",
    "\n",
    "- Useful for structured retrieval → you can search/filter not only by content but also by context (e.g., “all text under Hiking”).\n",
    "\n",
    "- Nested headers get accumulated in the metadata dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a90d0",
   "metadata": {},
   "source": [
    "### TokenTextSplitter\n",
    " breaks text into chunks based on tokens (the units an LLM processes), not words or characters. This ensures chunks align with the model’s tokenizer and stay within token limits.\n",
    "\n",
    "1. Suitable Scenarios\n",
    "\n",
    "- Preparing long documents so each chunk fits inside the model’s token window.\n",
    "\n",
    "- Splitting text before generating embeddings for retrieval (RAG).\n",
    "\n",
    "- Preserving context with controlled token-based overlap between chunks.\n",
    "\n",
    "- When working with texts that include special tokens (e.g., ``), where tokenization accuracy matters.\n",
    "\n",
    "2. When Not to Use\n",
    "\n",
    "- If you need human-friendly splits (e.g., by paragraph, sentence, or section).\n",
    "\n",
    "- When preserving semantic meaning is more important than staying strictly within token boundaries.\n",
    "\n",
    "- Since TokenTextSplitter may cut within a sentence or paragraph, it can break context and reduce readability.\n",
    "\n",
    "\n",
    "\n",
    "We will try the same text we used above, for easier comparison\n",
    "\n",
    "text1, text2, text3, and more_text variables from previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1deaf2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'def',\n",
       " 'gh',\n",
       " 'ij',\n",
       " 'kl',\n",
       " 'mn',\n",
       " 'op',\n",
       " 'q',\n",
       " 'r',\n",
       " 'st',\n",
       " 'uv',\n",
       " 'w',\n",
       " 'xy',\n",
       " 'z']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "token_split_gpt_2 = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "token_split_gpt_2.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c35eb",
   "metadata": {},
   "source": [
    "What is the token splitter doing now? * Note: the default tokenizer in the TokenTextSplitter is gpt-2.\n",
    "\n",
    "How the GPT-2 tokenizer works\n",
    "\n",
    "\n",
    "- GPT-2 uses Byte Pair Encoding (BPE) with a fixed vocabulary (~50,000 tokens).\n",
    "\n",
    "- It always tries to match the longest possible token in the vocabulary.\n",
    "\n",
    "- It starts with a base vocabulary of all single-byte characters (so every lowercase letter \"a\"–\"z\" exists as its own token).\n",
    "\n",
    "- During training, it merges frequent character pairs into new tokens (e.g., \"th\", \"ing\", \"er\", …).\n",
    "\n",
    "So when text is tokenized:\n",
    "\n",
    "- If a string can be matched by a longer token in the vocabulary, it’s grouped together.\n",
    "\n",
    "- If not, it falls back to single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8962b77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moh', 'amed', ' Ad', 'el', ' Hassan', ' Is', 'm', 'ai', 'el']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c32f1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moh',\n",
       " 'amed',\n",
       " ' Ad',\n",
       " 'el',\n",
       " ' Hassan',\n",
       " ' Is',\n",
       " 'm',\n",
       " 'ai',\n",
       " 'el',\n",
       " '\\n',\n",
       " 'L',\n",
       " 'ives',\n",
       " ' in',\n",
       " ' Cairo',\n",
       " '\\n',\n",
       " 'Works',\n",
       " ' as',\n",
       " ' an',\n",
       " ' AI',\n",
       " ' engineer',\n",
       " '\\n',\n",
       " 'Enjoy',\n",
       " 's',\n",
       " ' AI',\n",
       " ',',\n",
       " ' Python',\n",
       " ',',\n",
       " ' and',\n",
       " ' teaching']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc6fe3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " ' reading',\n",
       " ' documents',\n",
       " ',',\n",
       " ' readers',\n",
       " ' rely',\n",
       " ' on',\n",
       " ' structure',\n",
       " ' to',\n",
       " ' understand',\n",
       " ' the',\n",
       " ' flow',\n",
       " ' of',\n",
       " ' information',\n",
       " '.',\n",
       " ' Head',\n",
       " 'ings',\n",
       " ',',\n",
       " ' sub',\n",
       " 'head',\n",
       " 'ings',\n",
       " ',',\n",
       " ' and',\n",
       " ' paragraphs',\n",
       " ' provide',\n",
       " ' sign',\n",
       " 'posts',\n",
       " ' that',\n",
       " ' guide',\n",
       " ' the',\n",
       " ' reader',\n",
       " ' through',\n",
       " ' the',\n",
       " ' material',\n",
       " '.',\n",
       " ' For',\n",
       " ' instance',\n",
       " ',',\n",
       " ' a',\n",
       " ' heading',\n",
       " ' may',\n",
       " ' introduce',\n",
       " ' a',\n",
       " ' new',\n",
       " ' topic',\n",
       " ',',\n",
       " ' while',\n",
       " ' a',\n",
       " ' sub',\n",
       " 'heading',\n",
       " ' narrow',\n",
       " 's',\n",
       " ' the',\n",
       " ' focus',\n",
       " ' to',\n",
       " ' a',\n",
       " ' detail',\n",
       " '.',\n",
       " ' This',\n",
       " ' organization',\n",
       " ' helps',\n",
       " ' readers',\n",
       " ' follow',\n",
       " ' complex',\n",
       " ' ideas',\n",
       " ' step',\n",
       " ' by',\n",
       " ' step',\n",
       " '.',\n",
       " ' ',\n",
       " '\\n\\n',\n",
       " ' ',\n",
       " ' Format',\n",
       " 'ting',\n",
       " ' cues',\n",
       " ' like',\n",
       " ' bold',\n",
       " ' or',\n",
       " ' ital',\n",
       " 'ic',\n",
       " ' text',\n",
       " ' also',\n",
       " ' add',\n",
       " ' meaning',\n",
       " '.',\n",
       " ' They',\n",
       " ' emphasize',\n",
       " ' important',\n",
       " ' words',\n",
       " ' or',\n",
       " ' phrases',\n",
       " ' that',\n",
       " ' the',\n",
       " ' writer',\n",
       " ' wants',\n",
       " ' to',\n",
       " ' highlight',\n",
       " '.',\n",
       " ' Lists',\n",
       " ',',\n",
       " ' whether',\n",
       " ' numbered',\n",
       " ' or',\n",
       " ' bul',\n",
       " 'leted',\n",
       " ',',\n",
       " ' show',\n",
       " ' relationships',\n",
       " ' between',\n",
       " ' items',\n",
       " ' in',\n",
       " ' a',\n",
       " ' clear',\n",
       " ' manner',\n",
       " '.',\n",
       " ' Alt',\n",
       " 'ogether',\n",
       " ',',\n",
       " ' these',\n",
       " ' features',\n",
       " ' create',\n",
       " ' a',\n",
       " ' document',\n",
       " ' that',\n",
       " ' is',\n",
       " ' easier',\n",
       " ' to',\n",
       " ' scan',\n",
       " ',',\n",
       " ' read',\n",
       " ',',\n",
       " ' and',\n",
       " ' comprehend',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_split_gpt_2.split_text(more_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f46ea",
   "metadata": {},
   "source": [
    "More infor about how tokens correspond to text see this post from OpenAI for more details on how tokens are counted and how they correspond to text.\n",
    "\n",
    "According to the OpenAI post, the approximate token counts for English text are as follows:\n",
    "\n",
    "1 token ~= 4 chars in English\n",
    "1 token ~= ¾ words\n",
    "100 tokens ~= 75 words\n",
    "\n",
    "References\n",
    "\n",
    "- [langchain Tokens](https://python.langchain.com/docs/concepts/tokens/)\n",
    "- [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ad816",
   "metadata": {},
   "source": [
    "## RAG Comparison: All 4 Splitting Techniques\n",
    "\n",
    "### Setup for Comparison\n",
    "Let's create a simple test to compare how each splitter performs with the same RAG query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3fa042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text for comparison\n",
    "\n",
    "# load the pdf again\n",
    "loader = PyPDFLoader('D:\\Langchain\\Langchain-getting-started\\machinelearning-lecture01.pdf')\n",
    "test_text = loader.load()\n",
    "\n",
    "# Initialize all splitters with same parameters\n",
    "chunk_size = 450\n",
    "chunk_overlap = 0\n",
    "\n",
    "character_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap, \n",
    "    separator=' '\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb748110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING PERSISTENT VECTOR STORES ===\n",
      "Creating vector stores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector stores created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create persistent vector stores for all splitting techniques\n",
    "print(\"\\n=== CREATING PERSISTENT VECTOR STORES ===\")\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "# Create directories for each vector store\n",
    "db_dirs = {\n",
    "    \"character\": os.path.join(current_dir, \"db\", \"char_db_test\"),\n",
    "    \"recursive\": os.path.join(current_dir, \"db\", \"recursive_db_test\"), \n",
    "    \"token\": os.path.join(current_dir, \"db\", \"token_db_test\"),\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for db_dir in db_dirs.values():\n",
    "    os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "# Initialize embedding function\n",
    "embedding = OllamaEmbeddings(model='nomic-embed-text:v1.5')\n",
    "\n",
    "# Create documents for each splitter (except markdown which already has documents)\n",
    "character_docs = character_splitter.split_documents(test_text)\n",
    "recursive_docs = recursive_splitter.split_documents(test_text)\n",
    "token_docs = token_splitter.split_documents(test_text)\n",
    "\n",
    "# Create vector stores\n",
    "print(\"Creating vector stores...\")\n",
    "char_db = Chroma.from_documents(documents=character_docs, embedding=embedding, persist_directory=db_dirs[\"character\"])\n",
    "recursive_db = Chroma.from_documents(documents=recursive_docs, embedding=embedding, persist_directory=db_dirs[\"recursive\"])\n",
    "token_db = Chroma.from_documents(documents=token_docs, embedding=embedding, persist_directory=db_dirs[\"token\"])\n",
    "\n",
    "print(\"Vector stores created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a964968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING ALL VECTOR STORES WITH SAME QUERY ===\n",
      "\n",
      "--- CREATING CHARACTER VECTOR STORE ---\n",
      "\n",
      "--- CHARACTER DB DONE ---\n",
      "\n",
      "--- CREATING RECURSIVE VECTOR STORE ---\n",
      "\n",
      "--- RECURSIVE DB DONE ---\n",
      "\n",
      "--- CREATING TOKEN VECTOR STORE ---\n",
      "\n",
      "--- TOKEN DB DONE ---\n",
      "\n",
      "Number of chunks in each store:\n",
      "Character splitter: 144 chunks\n",
      "Recursive splitter: 157 chunks\n",
      "Token splitter: 42 chunks\n"
     ]
    }
   ],
   "source": [
    "# Test all vector stores with the same query\n",
    "print(\"\\n=== TESTING ALL VECTOR STORES WITH SAME QUERY ===\")\n",
    "query = \"What are the topics covered in the lecture?\"\n",
    "\n",
    "# Function to test and display results\n",
    "def test_vector_store(db, db_name, query, k=3):\n",
    "    print(f\"\\n--- CREATING {db_name.upper()} VECTOR STORE ---\")\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    print(f\"\\n--- {db_name.upper()} DB DONE ---\")\n",
    "    return results\n",
    "\n",
    "# Test all vector stores\n",
    "char_results = test_vector_store(char_db, \"character\", query)\n",
    "recursive_results = test_vector_store(recursive_db, \"recursive\", query) \n",
    "token_results = test_vector_store(token_db, \"token\", query)\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\nNumber of chunks in each store:\")\n",
    "print(f\"Character splitter: {len(character_docs)} chunks\")\n",
    "print(f\"Recursive splitter: {len(recursive_docs)} chunks\")\n",
    "print(f\"Token splitter: {len(token_docs)} chunks\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ddb5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== All retrieved chunks for character splitter ===\n",
      "\n",
      "Chunk 1:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay? \n",
      "So that was all I had from logistics. Before we move on to\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 2:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay? \n",
      "So that was all I had from logistics. Before we move on to\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 3:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay? \n",
      "So that was all I had from logistics. Before we move on to\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "=== All retrieved chunks for recursive splitter ===\n",
      "\n",
      "Chunk 1:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 2:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 3:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "=== All retrieved chunks for token splitter ===\n",
      "\n",
      "Chunk 1:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?  \n",
      "So that was all I had from logistics. Before we move on to start talking a bit about \n",
      "machine learning, let me check what questions you have. Yeah?  \n",
      "Student : [Inaudible] R or something like that?  \n",
      "Instructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \n",
      "welcome to use R, but I would strongly advise against it, mainly because in the last \n",
      "problem set, we actually supply some code that will run in Octave but that would be \n",
      "somewhat painful for you to translate into R yourself. So for your other assignments, if \n",
      "you wanna submit a solution in R, that's fine. But I think MATLAB is actually totally \n",
      "worth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \n",
      "bit more often for various reasons. Yeah?  \n",
      "Student : For the [inaudible] project [inaudible]?  \n",
      "Instructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \n",
      "groups of three, or you're welcome to do it by yourself or in groups of two. Grading is the \n",
      "same regardless of the group size, so with a larger group, you probably — I recommend \n",
      "trying to form a team, but it's actually totally fine to do it in a smaller group if you want.  \n",
      "Student : [Inaudible] what language [inaudible]?  \n",
      "Instructor (Andrew Ng): So let's see. There is no C programming in this class other \n",
      "than any that you may choose to do yourself in\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 2:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?  \n",
      "So that was all I had from logistics. Before we move on to start talking a bit about \n",
      "machine learning, let me check what questions you have. Yeah?  \n",
      "Student : [Inaudible] R or something like that?  \n",
      "Instructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \n",
      "welcome to use R, but I would strongly advise against it, mainly because in the last \n",
      "problem set, we actually supply some code that will run in Octave but that would be \n",
      "somewhat painful for you to translate into R yourself. So for your other assignments, if \n",
      "you wanna submit a solution in R, that's fine. But I think MATLAB is actually totally \n",
      "worth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \n",
      "bit more often for various reasons. Yeah?  \n",
      "Student : For the [inaudible] project [inaudible]?  \n",
      "Instructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \n",
      "groups of three, or you're welcome to do it by yourself or in groups of two. Grading is the \n",
      "same regardless of the group size, so with a larger group, you probably — I recommend \n",
      "trying to form a team, but it's actually totally fine to do it in a smaller group if you want.  \n",
      "Student : [Inaudible] what language [inaudible]?  \n",
      "Instructor (Andrew Ng): So let's see. There is no C programming in this class other \n",
      "than any that you may choose to do yourself in\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n",
      "\n",
      "Chunk 3:\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?  \n",
      "So that was all I had from logistics. Before we move on to start talking a bit about \n",
      "machine learning, let me check what questions you have. Yeah?  \n",
      "Student : [Inaudible] R or something like that?  \n",
      "Instructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \n",
      "welcome to use R, but I would strongly advise against it, mainly because in the last \n",
      "problem set, we actually supply some code that will run in Octave but that would be \n",
      "somewhat painful for you to translate into R yourself. So for your other assignments, if \n",
      "you wanna submit a solution in R, that's fine. But I think MATLAB is actually totally \n",
      "worth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \n",
      "bit more often for various reasons. Yeah?  \n",
      "Student : For the [inaudible] project [inaudible]?  \n",
      "Instructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \n",
      "groups of three, or you're welcome to do it by yourself or in groups of two. Grading is the \n",
      "same regardless of the group size, so with a larger group, you probably — I recommend \n",
      "trying to form a team, but it's actually totally fine to do it in a smaller group if you want.  \n",
      "Student : [Inaudible] what language [inaudible]?  \n",
      "Instructor (Andrew Ng): So let's see. There is no C programming in this class other \n",
      "than any that you may choose to do yourself in\n",
      "Metadata: {'page': 9, 'source': 'D:\\\\Langchain\\\\Langchain-getting-started\\\\machinelearning-lecture01.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# Show all retrieved chunks for every splitting technique\n",
    "\n",
    "def show_all_retrieved_chunks(results, splitter_name):\n",
    "    print(f\"\\n=== All retrieved chunks for {splitter_name} splitter ===\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\nChunk {i}:\")\n",
    "        print(doc.page_content)\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "show_all_retrieved_chunks(char_results, \"character\")\n",
    "show_all_retrieved_chunks(recursive_results, \"recursive\")\n",
    "show_all_retrieved_chunks(token_results, \"token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffbc8e",
   "metadata": {},
   "source": [
    "### Analysis of Results\n",
    "\n",
    "This comparison demonstrates how each splitter divides the same content and performs in a RAG scenario:\n",
    "\n",
    "**Key observations to look for:**\n",
    "- **Character splitter**: May split in the middle of sentences, potentially breaking context\n",
    "- **Recursive splitter**: Tries to preserve semantic boundaries (paragraphs → sentences → words)\n",
    "- **Token splitter**: Respects token boundaries but may break words or semantic units\n",
    "- **Markdown splitter**: Preserves header structure and maintains contextual relationships\n",
    "\n",
    "**Expected outcomes:**\n",
    "- The query \"What are the main types of machine learning?\" should retrieve chunks containing information about:\n",
    "  - Supervised learning\n",
    "  - Unsupervised learning  \n",
    "  - Reinforcement learning\n",
    "  - Deep learning\n",
    "\n",
    "**Performance metrics:**\n",
    "- Number of chunks generated by each approach\n",
    "- Relevance of retrieved chunks to the query\n",
    "- Preservation of metadata (especially for markdown splitting)\n",
    "- Context preservation across chunk boundaries\n",
    "\n",
    "\n",
    "You decide how well each technique preserves semantic context and retrieves relevant information about the asked query.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
